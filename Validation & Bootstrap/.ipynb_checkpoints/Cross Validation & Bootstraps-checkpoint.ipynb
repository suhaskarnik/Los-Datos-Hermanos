{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Cross Validation?\n",
    "\n",
    "* In general the validation set approach (i.e train on a subset of data and test on the remaining data) has the drawback that the error can vary a lot depending on what is and what is not in the training set. This increases the **bias** of the validation set approach\n",
    "* Also, in general the model ends up getting trained on lesser data, which tends to reduce the performance of models\n",
    "\n",
    "Cross-Validation tries to address these concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ISLR' was built under R version 3.3.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.3.3\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(ISLR)\n",
    "library(dplyr)\n",
    "set.seed(2)\n",
    "train=sample(392,196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "23.2955851508862"
      ],
      "text/latex": [
       "23.2955851508862"
      ],
      "text/markdown": [
       "23.2955851508862"
      ],
      "text/plain": [
       "[1] 23.29559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit=lm(mpg~horsepower,data=Auto,subset=train)\n",
    "\n",
    "# Calculate the test MSE\n",
    "mean((Auto$mpg-predict(lm.fit,Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try the same thing for Polynomial regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "18.9012408317778"
      ],
      "text/latex": [
       "18.9012408317778"
      ],
      "text/markdown": [
       "18.9012408317778"
      ],
      "text/plain": [
       "[1] 18.90124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "19.2573982608642"
      ],
      "text/latex": [
       "19.2573982608642"
      ],
      "text/markdown": [
       "19.2573982608642"
      ],
      "text/plain": [
       "[1] 19.2574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)\n",
    "mean((Auto$mpg-predict(lm.fit2,Auto))[-train]^2)\n",
    "\n",
    "lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)\n",
    "mean((Auto$mpg-predict(lm.fit3,Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOCV\n",
    "\n",
    "* LOOCV takes the first element as the validation set, with the rest of the data as the training set. It then trains or fits the model on the training set, and validates against the validation set\n",
    "* And then repeats the process with the second element as the validation set and so on\n",
    "* This results in a very low bias, as each training set contains almost all the data\n",
    "\n",
    "LOOCV can be automatically calculated by using the `glm` and `cv.glm` functions.\n",
    "\n",
    "`glm()` without any \"family\" arg performs linear regression. Here we'll use `glm` instead of `lm` as that can be later used with `cv.glm()`\n",
    "\n",
    "`cv.glm` is in the `boot` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>39.9358610211705</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.157844733353654</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 39.9358610211705\n",
       "\\item[horsepower] -0.157844733353654\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   39.9358610211705horsepower\n",
       ":   -0.157844733353654\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 39.9358610  -0.1578447 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(boot)\n",
    "glm.fit=glm(mpg~horsepower,data=Auto)\n",
    "coef(glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cv.glm` performs cross-validation. It's `delta` vector gives the X-validation results.\n",
    "\n",
    "$$CV_{(x)} = \\frac{1}{n}\\sum_{i=1}^n MSE_i$$\n",
    "\n",
    "The result of cv.glm contains multiple components. The `delta` component is a vector of two: the first element is the raw X-validation estimate of prediction error, the second element is the bias-corrected X-validation estimate. \n",
    "\n",
    "Need to understand more what they actually mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2315135179292</li>\n",
       "\t<li>24.2311440937562</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2315135179292\n",
       "\\item 24.2311440937562\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2315135179292\n",
       "2. 24.2311440937562\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 24.23151 24.23114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.err=cv.glm(Auto,glm.fit)\n",
    "\n",
    "cv.err$delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both elements are almost the same.\n",
    "\n",
    "Let's try LOOC for several polynomial fits (this could take some time). Hint: in notebook mode, the cell shows a [\\*] if it is still executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2315135179292</li>\n",
       "\t<li>19.2482131244897</li>\n",
       "\t<li>19.334984064029</li>\n",
       "\t<li>19.4244303104303</li>\n",
       "\t<li>19.0332138547041</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2315135179292\n",
       "\\item 19.2482131244897\n",
       "\\item 19.334984064029\n",
       "\\item 19.4244303104303\n",
       "\\item 19.0332138547041\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2315135179292\n",
       "2. 19.2482131244897\n",
       "3. 19.334984064029\n",
       "4. 19.4244303104303\n",
       "5. 19.0332138547041\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 24.23151 19.24821 19.33498 19.42443 19.03321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.error = rep(0,5)\n",
    "\n",
    "for(i in 1:5){\n",
    "    glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
    "    cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]\n",
    "}\n",
    "cv.error\n",
    "plot(1:5,cv.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's a sharp drop from order=1(linear) to order=2(quadratic) but not much of a drop after that. So a quad fit would work best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "In K-Fold, the dataset is split randomly into K groups (or folds). Then the first set is taken as the validation set, while the other `k-1` sets are used to fit the model. Then the process is repeated with the second set as the validation set. The CV estimate then is the average of the MSEs for each of the folds\n",
    "\n",
    "* LOOCV is essentially K-fold, with K=N\n",
    "* LOOCV requires regression to be performed N times for N predictions\n",
    "* K-Fold requires regression to be done K times, so there is a computational advantage\n",
    "* The LOOCV suffers from extremely low bias, and K-Fold will have slightly higher bias (though not as much as simple validation set method)\n",
    "* LOOCV has a very high variance though. Because LOOCV averages the output of N models, each of which is trained on almost identical datasets. So the outputs have high +ve correlation. The mean of many highly correlated quantities has higher variance than the mean of quantities that aren't highly correlated\n",
    "* The datasets for K-Fold have lesser +ve correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2051967567753</li>\n",
       "\t<li>19.1892390663471</li>\n",
       "\t<li>19.3066158967501</li>\n",
       "\t<li>19.3379909004929</li>\n",
       "\t<li>18.8791148363354</li>\n",
       "\t<li>19.0210341885228</li>\n",
       "\t<li>18.8960903802809</li>\n",
       "\t<li>19.7120146188182</li>\n",
       "\t<li>18.9514005667302</li>\n",
       "\t<li>19.501959228555</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2051967567753\n",
       "\\item 19.1892390663471\n",
       "\\item 19.3066158967501\n",
       "\\item 19.3379909004929\n",
       "\\item 18.8791148363354\n",
       "\\item 19.0210341885228\n",
       "\\item 18.8960903802809\n",
       "\\item 19.7120146188182\n",
       "\\item 18.9514005667302\n",
       "\\item 19.501959228555\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2051967567753\n",
       "2. 19.1892390663471\n",
       "3. 19.3066158967501\n",
       "4. 19.3379909004929\n",
       "5. 18.8791148363354\n",
       "6. 19.0210341885228\n",
       "7. 18.8960903802809\n",
       "8. 19.7120146188182\n",
       "9. 18.9514005667302\n",
       "10. 19.501959228555\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 24.20520 19.18924 19.30662 19.33799 18.87911 19.02103 18.89609 19.71201\n",
       " [9] 18.95140 19.50196"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\noq8AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(17)\n",
    "\n",
    "cv.error.10=rep(1,10)\n",
    "\n",
    "for(i in 1:10){\n",
    "    glm.fit=glm(mpg~poly(horsepower,i),data=Auto)\n",
    "    cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]\n",
    "}\n",
    "\n",
    "cv.error.10\n",
    "plot(1:10,cv.error.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are rather similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "\n",
    "\n",
    "### Simple Explanation\n",
    "Consider a dataset `Z` with `n=3` obs. Randomly select `n` observations *with replacement* to produce a bootstrap dataset. So the same obs can occur multiple times in the bootstrap dataset\n",
    "\n",
    "Let's say $Z=[1,2,3]$\n",
    "\n",
    "Then some Bootstrap sets could be $Z^{*1} = [1,1,3]$ and $Z^{*2} = [1,3,2]$\n",
    "\n",
    "Produce B different Bootstrap sets $Z^{*1},Z^{*2},Z^{*3}...Z^{*B}$\n",
    "\n",
    "Using each set, produce an estimated prediction $\\hat{\\alpha}^{*1},\\hat{\\alpha}^{*2},\\hat{\\alpha}^{*3}...\\hat{\\alpha}^{*B}$\n",
    "\n",
    "Then the Std Error is:\n",
    "\n",
    "$$SE_B(\\hat{\\alpha}) = \\sqrt{ \\frac{1}{B-1} \\sum_{r=1}^B(\\hat{\\alpha}^{*r}- \\frac{1}{B} \\sum_{s=1}^B {\\alpha}^{*s} )^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap for Linear Reg\n",
    "\n",
    "Linear Reg equation:\n",
    "\n",
    "$$horsepower = \\beta_0 + \\beta_1mpg + \\epsilon$$\n",
    "\n",
    "Using Bootstraps, lets calculate the variability of the estimates for $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "First create a function that takes the Auto data, indices for observations and spits out intercept and slope estimates ($\\hat{\\beta_0}$ and $\\hat{\\beta_1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Auto, statistic = boot.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "      original        bias    std. error\n",
       "t1* 39.9358610  0.0118427139 0.871057282\n",
       "t2* -0.1578447 -0.0002632441 0.007537443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot.fn = function(data,index)\n",
    "    coef(lm(mpg~horsepower,data=Auto,subset = index))\n",
    "\n",
    "\n",
    "# Create 1000 data sets by sampling from Auto, apply the boot.fn on that and assess the variability of the results\n",
    "boot(Auto,boot.fn,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicates that $SE(\\hat{\\beta_0})=0.871$ and $SE(\\hat{\\beta_1})=0.008$\n",
    "\n",
    "Let's also compare that with the Std Errors obtained by `lm` itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>39.9358610   </td><td>0.717498656  </td><td> 55.65984    </td><td>1.220362e-187</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-0.1578447   </td><td>0.006445501  </td><td>-24.48914    </td><td> 7.031989e-81</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & 39.9358610    & 0.717498656   &  55.65984     & 1.220362e-187\\\\\n",
       "\thorsepower & -0.1578447    & 0.006445501   & -24.48914     &  7.031989e-81\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(>|t|) | \n",
       "|---|---|\n",
       "| (Intercept) | 39.9358610    | 0.717498656   |  55.65984     | 1.220362e-187 | \n",
       "| horsepower | -0.1578447    | 0.006445501   | -24.48914     |  7.031989e-81 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            Estimate   Std. Error  t value   Pr(>|t|)     \n",
       "(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\n",
       "horsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lm(mpg~horsepower,data=Auto))$coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Std Errors here (0.717 and 0.006) differ from the Bootstrap errors. However, that isn't a problem with Bootstrap.\n",
    "\n",
    "The `lm` calculations of Std Error are dependent on the assumptions that\n",
    "* The data has a linear relationship\n",
    "* The $x_i$ are fixed, and all variability comes from the $\\epsilon_i$ terms\n",
    "\n",
    "Bootstrap doesn't assume that, so it is likely to be accurate\n",
    "\n",
    "Let's also try Bootstrap with the quadratic fit, because that has earlier proved to be more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Auto, statistic = boot.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "        original        bias     std. error\n",
       "t1* 56.900099702  6.098115e-03 2.0944855842\n",
       "t2* -0.466189630 -1.777108e-04 0.0334123802\n",
       "t3*  0.001230536  1.324315e-06 0.0001208339"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>56.900099702 </td><td>1.8004268063 </td><td> 31.60367    </td><td>1.740911e-109</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-0.466189630 </td><td>0.0311246171 </td><td>-14.97816    </td><td> 2.289429e-40</td></tr>\n",
       "\t<tr><th scope=row>I(horsepower^2)</th><td> 0.001230536 </td><td>0.0001220759 </td><td> 10.08009    </td><td> 2.196340e-21</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & 56.900099702  & 1.8004268063  &  31.60367     & 1.740911e-109\\\\\n",
       "\thorsepower & -0.466189630  & 0.0311246171  & -14.97816     &  2.289429e-40\\\\\n",
       "\tI(horsepower\\textasciicircum{}2) &  0.001230536  & 0.0001220759  &  10.08009     &  2.196340e-21\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(>|t|) | \n",
       "|---|---|---|\n",
       "| (Intercept) | 56.900099702  | 1.8004268063  |  31.60367     | 1.740911e-109 | \n",
       "| horsepower | -0.466189630  | 0.0311246171  | -14.97816     |  2.289429e-40 | \n",
       "| I(horsepower^2) |  0.001230536  | 0.0001220759  |  10.08009     |  2.196340e-21 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                Estimate     Std. Error   t value   Pr(>|t|)     \n",
       "(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\n",
       "horsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\n",
       "I(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "boot.fn = function(data,index)\n",
    "    coef(lm(mpg~horsepower+I(horsepower^2),data=Auto,subset = index))\n",
    "\n",
    "\n",
    "# Create 1000 data sets by sampling from Auto, apply the boot.fn on that and assess the variability of the results\n",
    "boot(Auto,boot.fn,1000)\n",
    "    \n",
    "summary (lm(mpg∼horsepower +I(horsepower ^2) ,data=Auto))$coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap on Portfolio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.57583207459283"
      ],
      "text/latex": [
       "0.57583207459283"
      ],
      "text/markdown": [
       "0.57583207459283"
      ],
      "text/plain": [
       "[1] 0.5758321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha.fn=function(data,index){\n",
    "    X=data$X[index]\n",
    "    Y=data$Y[index]\n",
    "    ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))\n",
    "}\n",
    "\n",
    "alpha.fn(Portfolio,1:100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a bootstrap with 1000 estimates for $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Portfolio, statistic = alpha.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "     original       bias    std. error\n",
       "t1* 0.5758321 6.936399e-05  0.08868935"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "\n",
    "boot(Portfolio,alpha.fn,R=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So estimation of $\\hat{\\alpha}=0.5758, SE(\\hat{\\alpha})=0.0886$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
