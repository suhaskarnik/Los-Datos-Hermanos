{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict today's direction based on all other variables\n",
    "This notebook follows the lab exercise for ISLR Classification Chapter\n",
    "\n",
    "Lag1-5 are the % returns of the previous 5 days\n",
    "\n",
    "Create a correlation matrix between the Variables (except Direction, which is qualititative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "attach(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Year</th><th scope=col>Lag1</th><th scope=col>Lag2</th><th scope=col>Lag3</th><th scope=col>Lag4</th><th scope=col>Lag5</th><th scope=col>Volume</th><th scope=col>Today</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Year</th><td>1.00000000  </td><td> 0.029699649</td><td> 0.030596422</td><td> 0.033194581</td><td> 0.035688718</td><td> 0.029787995</td><td> 0.53900647 </td><td> 0.030095229</td></tr>\n",
       "\t<tr><th scope=row>Lag1</th><td>0.02969965  </td><td> 1.000000000</td><td>-0.026294328</td><td>-0.010803402</td><td>-0.002985911</td><td>-0.005674606</td><td> 0.04090991 </td><td>-0.026155045</td></tr>\n",
       "\t<tr><th scope=row>Lag2</th><td>0.03059642  </td><td>-0.026294328</td><td> 1.000000000</td><td>-0.025896670</td><td>-0.010853533</td><td>-0.003557949</td><td>-0.04338321 </td><td>-0.010250033</td></tr>\n",
       "\t<tr><th scope=row>Lag3</th><td>0.03319458  </td><td>-0.010803402</td><td>-0.025896670</td><td> 1.000000000</td><td>-0.024051036</td><td>-0.018808338</td><td>-0.04182369 </td><td>-0.002447647</td></tr>\n",
       "\t<tr><th scope=row>Lag4</th><td>0.03568872  </td><td>-0.002985911</td><td>-0.010853533</td><td>-0.024051036</td><td> 1.000000000</td><td>-0.027083641</td><td>-0.04841425 </td><td>-0.006899527</td></tr>\n",
       "\t<tr><th scope=row>Lag5</th><td>0.02978799  </td><td>-0.005674606</td><td>-0.003557949</td><td>-0.018808338</td><td>-0.027083641</td><td> 1.000000000</td><td>-0.02200231 </td><td>-0.034860083</td></tr>\n",
       "\t<tr><th scope=row>Volume</th><td>0.53900647  </td><td> 0.040909908</td><td>-0.043383215</td><td>-0.041823686</td><td>-0.048414246</td><td>-0.022002315</td><td> 1.00000000 </td><td> 0.014591823</td></tr>\n",
       "\t<tr><th scope=row>Today</th><td>0.03009523  </td><td>-0.026155045</td><td>-0.010250033</td><td>-0.002447647</td><td>-0.006899527</td><td>-0.034860083</td><td> 0.01459182 </td><td> 1.000000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       "  & Year & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today\\\\\n",
       "\\hline\n",
       "\tYear & 1.00000000   &  0.029699649 &  0.030596422 &  0.033194581 &  0.035688718 &  0.029787995 &  0.53900647  &  0.030095229\\\\\n",
       "\tLag1 & 0.02969965   &  1.000000000 & -0.026294328 & -0.010803402 & -0.002985911 & -0.005674606 &  0.04090991  & -0.026155045\\\\\n",
       "\tLag2 & 0.03059642   & -0.026294328 &  1.000000000 & -0.025896670 & -0.010853533 & -0.003557949 & -0.04338321  & -0.010250033\\\\\n",
       "\tLag3 & 0.03319458   & -0.010803402 & -0.025896670 &  1.000000000 & -0.024051036 & -0.018808338 & -0.04182369  & -0.002447647\\\\\n",
       "\tLag4 & 0.03568872   & -0.002985911 & -0.010853533 & -0.024051036 &  1.000000000 & -0.027083641 & -0.04841425  & -0.006899527\\\\\n",
       "\tLag5 & 0.02978799   & -0.005674606 & -0.003557949 & -0.018808338 & -0.027083641 &  1.000000000 & -0.02200231  & -0.034860083\\\\\n",
       "\tVolume & 0.53900647   &  0.040909908 & -0.043383215 & -0.041823686 & -0.048414246 & -0.022002315 &  1.00000000  &  0.014591823\\\\\n",
       "\tToday & 0.03009523   & -0.026155045 & -0.010250033 & -0.002447647 & -0.006899527 & -0.034860083 &  0.01459182  &  1.000000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Year | Lag1 | Lag2 | Lag3 | Lag4 | Lag5 | Volume | Today | \n",
       "|---|---|---|---|---|---|---|---|\n",
       "| Year | 1.00000000   |  0.029699649 |  0.030596422 |  0.033194581 |  0.035688718 |  0.029787995 |  0.53900647  |  0.030095229 | \n",
       "| Lag1 | 0.02969965   |  1.000000000 | -0.026294328 | -0.010803402 | -0.002985911 | -0.005674606 |  0.04090991  | -0.026155045 | \n",
       "| Lag2 | 0.03059642   | -0.026294328 |  1.000000000 | -0.025896670 | -0.010853533 | -0.003557949 | -0.04338321  | -0.010250033 | \n",
       "| Lag3 | 0.03319458   | -0.010803402 | -0.025896670 |  1.000000000 | -0.024051036 | -0.018808338 | -0.04182369  | -0.002447647 | \n",
       "| Lag4 | 0.03568872   | -0.002985911 | -0.010853533 | -0.024051036 |  1.000000000 | -0.027083641 | -0.04841425  | -0.006899527 | \n",
       "| Lag5 | 0.02978799   | -0.005674606 | -0.003557949 | -0.018808338 | -0.027083641 |  1.000000000 | -0.02200231  | -0.034860083 | \n",
       "| Volume | 0.53900647   |  0.040909908 | -0.043383215 | -0.041823686 | -0.048414246 | -0.022002315 |  1.00000000  |  0.014591823 | \n",
       "| Today | 0.03009523   | -0.026155045 | -0.010250033 | -0.002447647 | -0.006899527 | -0.034860083 |  0.01459182  |  1.000000000 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       Year       Lag1         Lag2         Lag3         Lag4        \n",
       "Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\n",
       "Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\n",
       "Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\n",
       "Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\n",
       "Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\n",
       "Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\n",
       "Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\n",
       "Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n",
       "       Lag5         Volume      Today       \n",
       "Year    0.029787995  0.53900647  0.030095229\n",
       "Lag1   -0.005674606  0.04090991 -0.026155045\n",
       "Lag2   -0.003557949 -0.04338321 -0.010250033\n",
       "Lag3   -0.018808338 -0.04182369 -0.002447647\n",
       "Lag4   -0.027083641 -0.04841425 -0.006899527\n",
       "Lag5    1.000000000 -0.02200231 -0.034860083\n",
       "Volume -0.022002315  1.00000000  0.014591823\n",
       "Today  -0.034860083  0.01459182  1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor(Smarket[,-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* There's no correlation between today's returns and LagN variables (close to zero)\n",
    "* The only meaningful relationship seems to be between Volume and Year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is denoted by the `family=binomial` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n",
       "    Volume, family = binomial, data = Smarket)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-1.446  -1.203   1.065   1.145   1.326  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept) -0.126000   0.240736  -0.523    0.601\n",
       "Lag1        -0.073074   0.050167  -1.457    0.145\n",
       "Lag2        -0.042301   0.050086  -0.845    0.398\n",
       "Lag3         0.011085   0.049939   0.222    0.824\n",
       "Lag4         0.009359   0.049974   0.187    0.851\n",
       "Lag5         0.010313   0.049511   0.208    0.835\n",
       "Volume       0.135441   0.158360   0.855    0.392\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1731.2  on 1249  degrees of freedom\n",
       "Residual deviance: 1727.6  on 1243  degrees of freedom\n",
       "AIC: 1741.6\n",
       "\n",
       "Number of Fisher Scoring iterations: 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmfit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)\n",
    "summary(glmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Smallest p-value is for Lag1. And the coef is negative, so if market moved up last day, then it will move down today\n",
    "* But its p-value is still relatively high ($>0.05$), so no clear evidence of association betw Direction and Lag1\n",
    "* The `contrasts(Direction)` function indicates that R has created a dummy variable with a 1 for `Up`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `predict` function to predict (duh!) the probability that the market will go up/down\n",
    "\n",
    "The `type=\"response\"` option tells R to output probabilities for $P(Y=1|X)$, as opposed to other info like the Logit\n",
    "\n",
    "As no data is supplied to the `predict` function, it will try to predict for the training data set itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.507084133395401</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.481467878454591</dd>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>0.481138835214201</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>0.515222355813022</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>0.510781162691538</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>0.506956460534911</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>0.492650874187038</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>0.509229158207377</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>0.517613526170958</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>0.488837779771376</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.507084133395401\n",
       "\\item[2] 0.481467878454591\n",
       "\\item[3] 0.481138835214201\n",
       "\\item[4] 0.515222355813022\n",
       "\\item[5] 0.510781162691538\n",
       "\\item[6] 0.506956460534911\n",
       "\\item[7] 0.492650874187038\n",
       "\\item[8] 0.509229158207377\n",
       "\\item[9] 0.517613526170958\n",
       "\\item[10] 0.488837779771376\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.5070841333954012\n",
       ":   0.4814678784545913\n",
       ":   0.4811388352142014\n",
       ":   0.5152223558130225\n",
       ":   0.5107811626915386\n",
       ":   0.5069564605349117\n",
       ":   0.4926508741870388\n",
       ":   0.5092291582073779\n",
       ":   0.51761352617095810\n",
       ":   0.488837779771376\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4         5         6         7         8 \n",
       "0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n",
       "        9        10 \n",
       "0.5176135 0.4888378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Up</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Down</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>Up</th><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & Up\\\\\n",
       "\\hline\n",
       "\tDown & 0\\\\\n",
       "\tUp & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Up | \n",
       "|---|---|\n",
       "| Down | 0 | \n",
       "| Up | 1 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     Up\n",
       "Down 0 \n",
       "Up   1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmprobs=predict(glmfit,type=\"response\")\n",
    "glmprobs[1:10]  #As Up is denoted by 1, these probs are for the market going up\n",
    "contrasts(Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict whether the market will go up or down, lets convert the probabilities into class labels\n",
    "\n",
    "First create a vector with 1250 <sub>(count of original dataset)</sub> \"Down\" values. \n",
    "\n",
    "We use a threshold of 0.5, so if the calculated prob > 0.5, then the market is assumed to move upwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Up'</li>\n",
       "\t<li>'Down'</li>\n",
       "\t<li>'Down'</li>\n",
       "\t<li>'Up'</li>\n",
       "\t<li>'Up'</li>\n",
       "\t<li>'Up'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Up'\n",
       "\\item 'Down'\n",
       "\\item 'Down'\n",
       "\\item 'Up'\n",
       "\\item 'Up'\n",
       "\\item 'Up'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Up'\n",
       "2. 'Down'\n",
       "3. 'Down'\n",
       "4. 'Up'\n",
       "5. 'Up'\n",
       "6. 'Up'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Up\"   \"Down\" \"Down\" \"Up\"   \"Up\"   \"Up\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmpred=rep(\"Down\",1250)\n",
    "glmpred[glmprobs>0.5]=\"Up\"\n",
    "head(glmpred)   #Look at some of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a **Confusion matrix** to determine how many obs were correctly classified\n",
    "\n",
    "`glmpred` contains 1250 computed predictions, and `Direction` from the DF contains 1250 actual direction values\n",
    "\n",
    "So this will tell how many observations match and how many don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Direction\n",
       "glmpred Down  Up\n",
       "   Down  145 141\n",
       "   Up    457 507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate Prediction Rate 0.5216"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.5216"
      ],
      "text/latex": [
       "0.5216"
      ],
      "text/markdown": [
       "0.5216"
      ],
      "text/plain": [
       "[1] 0.5216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(glmpred,Direction)\n",
    "cat('Accurate Prediction Rate', (507+145)/1250)\n",
    "mean(glmpred==Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Accurate predictions only 52% of the time\n",
    "* It doesn't look much better than random guessing\n",
    "* However, we tested the model on the **training** set, which leads to more optimistic results\n",
    "* This means the model may actually be performing worse\n",
    "* So let's now create a training set, containing data for years before 2005. Then we'll test with data 2005 onwards. The `.2005` variables contain the 2005 which will be our **test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>252</li>\n",
       "\t<li>9</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 252\n",
       "\\item 9\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 252\n",
       "2. 9\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 252   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=(Year<2005)\n",
    "Smarket.2005=Smarket[!train,]\n",
    "dim(Smarket.2005)\n",
    "Direction.2005=Direction[!train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `subset=train` arg which will tell the `glm` function to take only data subset specified by the `train` variable above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Direction.2005\n",
       "glmpred Down Up\n",
       "   Down   77 97\n",
       "   Up     34 44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.51984126984127"
      ],
      "text/latex": [
       "0.51984126984127"
      ],
      "text/markdown": [
       "0.51984126984127"
      ],
      "text/plain": [
       "[1] 0.5198413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glmfit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,family=binomial,data=Smarket,subset=train)\n",
    "\n",
    "glmprobs=predict(glmfit,Smarket.2005,type=\"response\")\n",
    "\n",
    "glmpred=rep(\"Down\",252)\n",
    "glmpred[glmprobs>0.5]=\"Up\"\n",
    "\n",
    "# Now compare predictions vs actual 2005 values\n",
    "table(glmpred,Direction.2005)\n",
    "\n",
    "mean(glmpred!=Direction.2005)  # Will give the total error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Error Rate: 52% \n",
    "\n",
    "So essentially, this is worse than simple coin-tossing\n",
    "\n",
    "Which makes sense, as it isn't plausible that the stock market can be reliably predicted by looking at previous days' prices\n",
    "\n",
    "It also makes sense because the p-values were not very impressive\n",
    "\n",
    "Let's try to remove some additional variables. It might help, as using superfluous predictors can cause deteriotation, as those predictors cause increase in variance without a corresponding decrease in bias.\n",
    "\n",
    "Let's try with just Lag1 and Lag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Direction.2005\n",
       "glmpred Down  Up\n",
       "   Down   35  35\n",
       "   Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Error Rate: 44.04762 %\n",
      "PPV: 58.24176 %\n",
      "Up Probability in test set:  55.95238 %"
     ]
    }
   ],
   "source": [
    "glmfit=glm(Direction~Lag1+Lag2,family=binomial,data=Smarket,subset=train)\n",
    "glmprobs=predict(glmfit,Smarket.2005,type=\"response\")\n",
    "\n",
    "glmpred=rep(\"Down\",252)\n",
    "glmpred[glmprobs>0.5]=\"Up\"\n",
    "\n",
    "# Now compare predictions vs actual 2005 values\n",
    "table(glmpred,Direction.2005)\n",
    "\n",
    "cat('Total Error Rate:',100*mean(glmpred!=Direction.2005),'%\\n')  # Will give the total error rate\n",
    "cat('PPV:',106/(106+76)*100,'%\\n')        # Will give the Positive Pred Val (PPV)\n",
    "cat('Up Probability in test set: ',mean(Direction.2005==\"Up\")*100,'%')  # How many times in the test set did the market move upwards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better. We get the positives *correct* 58% of the time. This is the Positive Pred Value (PPV)\n",
    "\n",
    "$$PPV = \\frac{\\sum True Positive}{\\sum All Positives} = 58.24\\% $$\n",
    "\n",
    "$$True Positive = 106 $$ \n",
    "$$All Positives = 106+76 $$\n",
    "\n",
    "\n",
    "But... in the test dataset, market moves up 56% of days. So if we just blindly \"predicted\" that that market would go Up each day, we would achieve the same \"accuracy\" \n",
    "\n",
    "Yet, let's try to see what predictions we get for a couple of combinations of Lag1 and Lag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.479146239171912</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.496093872956532</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.479146239171912\n",
       "\\item[2] 0.496093872956532\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.4791462391719122\n",
       ":   0.496093872956532\n",
       "\n"
      ],
      "text/plain": [
       "        1         2 \n",
       "0.4791462 0.4960939 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict (glmfit ,newdata=data.frame(Lag1=c(1.2 ,1.5) ,Lag2=c(1.1 , -0.8) ),type =\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see some diagnostic plots for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAABNTU1oaGh8fHx/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\nRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(glmfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "Function `lda` is part of the `MASS` library\n",
    "\n",
    "Syntax is identical to `lm` and `glm`\n",
    "\n",
    "LDA uses Bayes Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "            LD1\n",
       "Lag1 -0.6420190\n",
       "Lag2 -0.5135293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "ldafit=lda(Direction~Lag1+Lag2,data=Smarket ,subset =train)\n",
    "ldafit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### `Prior Probabilities`: $\\hat\\pi_{down} = 0.492, \\hat\\pi_{up} = 0.508$\n",
    "* Which means that in the training set, it goes up 50.8% of the time\n",
    "\n",
    "#### `Group Means`: Average of each predictor in each class\n",
    "* Group means for Down/Lag1 and Down/Lag2 are positive, which means that the market usually goes down if it went up (+ve) the previous two days\n",
    "* Group means for Up are negative, which means that the market usually goes up if it went down (-ve) the previous two days\n",
    "\n",
    "#### `Coefficients`: Linear combinations of `Lag1` and `Lag2` used for LDA Decision Rule\n",
    "If $-0.642 \\times Lag1 + (-0.514) \\times Lag2$ is large, then it predicts that the market will go ***Up***. Otherwise it predicts that the market will go ***Down***.\n",
    "\n",
    "Let's plot the LDA fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAA//9NTU1oaGh8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\ns08syQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(ldafit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` fn gives a list with 3 elements. `Class` gives the LDA predictions. `Posterior` is a matrix, the *k*th column contains the *posterior* probability that the obs belongs to the *k*th class. And `x` contains the linear discriminants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'class'</li>\n",
       "\t<li>'posterior'</li>\n",
       "\t<li>'x'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'class'\n",
       "\\item 'posterior'\n",
       "\\item 'x'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'class'\n",
       "2. 'posterior'\n",
       "3. 'x'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"class\"     \"posterior\" \"x\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "        Direction.2005\n",
       "ldaclass Down  Up\n",
       "    Down   35  35\n",
       "    Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 44 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "70"
      ],
      "text/latex": [
       "70"
      ],
      "text/markdown": [
       "70"
      ],
      "text/plain": [
       "[1] 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "70"
      ],
      "text/latex": [
       "70"
      ],
      "text/markdown": [
       "70"
      ],
      "text/plain": [
       "[1] 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ldapred=predict(ldafit,Smarket.2005)\n",
    "names(ldapred)\n",
    "\n",
    "ldaclass=ldapred$class   #Predictions\n",
    "table(ldaclass,Direction.2005)\n",
    "\n",
    "cat('Error Rate:',round(mean(ldaclass!=Direction.2005)*100),'%\\n')\n",
    "\n",
    "\n",
    "sum(ldapred$posterior[,\"Down\"]>=0.5)                      #The posterior probs of going Down, threshold = 0.5\n",
    "length(ldapred$class[ldapred$class==\"Down\"])\n",
    "\n",
    "sum(ldapred$posterior[,\"Down\"]>=0.9)                      #The posterior probs of going Down, threshold = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)\n",
    "Use `qda` function, syntax identical to LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qdafit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)\n",
    "qdafit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Similar to LDA fit, but there are no coefficients, as QDA doesn't have linear coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Direction.2005\n",
       "qdaclass Down  Up\n",
       "    Down   30  20\n",
       "    Up     81 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate:  40 %\n"
     ]
    }
   ],
   "source": [
    "qdaclass=predict(qdafit,Smarket.2005)$class\n",
    "\n",
    "table(qdaclass,Direction.2005)\n",
    "cat('Error rate: ',round(100*mean(qdaclass!=Direction.2005)),'%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Error rate is only 40%. Impressive as this is stock market data, which is very hard to predict. \n",
    "\n",
    "Possibly, QDA models the true nature of the fit better than LDA/LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "The `knn` fn is part of the `class` library\n",
    "\n",
    "Works differently. Instead of first fitting the model and then use it to make predictions, we can form predictions using a single command\n",
    "\n",
    "Four inputs needed:\n",
    "1. A matrix with predictors for the *training* data\n",
    "2. A matrix with predictors for the *test* data\n",
    "3. A vector with class labels for the *training* observations\n",
    "4. A value for *K*, i.e. the number of nearest neighbors to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(class)\n",
    "train.X=cbind(Lag1,Lag2)[train,]\n",
    "test.X=cbind(Lag1,Lag2)[!train,]\n",
    "train.Direction=Direction[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set a defined random seed. Because if KNN finds multiple observations tied as nearest neighbors, it will select one at random. So to ensure reproducibility of results, some constant seed should be set.\n",
    "\n",
    "Let's begin by setting *K*=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Direction.2005\n",
       "knnpred Down Up\n",
       "   Down   43 58\n",
       "   Up     68 83"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "50"
      ],
      "text/latex": [
       "50"
      ],
      "text/markdown": [
       "50"
      ],
      "text/plain": [
       "[1] 50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "knnpred=knn(train.X,test.X,train.Direction,k=1)\n",
    "\n",
    "table(knnpred,Direction.2005)\n",
    "\n",
    "(83+43)*100/252   # TruePositive+TrueNegative/Total% in Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Not too good, equivalent to a coin toss\n",
    "* Because K=1 results in high variance and flexibility\n",
    "* Let's try with K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Direction.2005\n",
       "knnpred Down Up\n",
       "   Down   48 54\n",
       "   Up     63 87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "53.5714285714286"
      ],
      "text/latex": [
       "53.5714285714286"
      ],
      "text/markdown": [
       "53.5714285714286"
      ],
      "text/plain": [
       "[1] 53.57143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knnpred=knn(train.X,test.X,train.Direction,k=3)\n",
    "\n",
    "table(knnpred,Direction.2005)\n",
    "\n",
    "(87+48)*100/252   # TruePositive+TrueNegative/Total% in Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare All 4 methods\n",
    "\n",
    "KPI for comparing is Accuracy, defined as \n",
    "$$Accuracy = \\frac {\\sum True Positive + \\sum True Positive}{\\sum Test Population}$$\n",
    "\n",
    "Based on these, QDA appears to provide the best fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>name</th><th scope=col>acc</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Logistic Regression</td><td>56                 </td></tr>\n",
       "\t<tr><td>LDA</td><td>56 </td></tr>\n",
       "\t<tr><td>QDA</td><td>60 </td></tr>\n",
       "\t<tr><td>KNN</td><td>54 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       " name & acc\\\\\n",
       "\\hline\n",
       "\t Logistic Regression & 56                 \\\\\n",
       "\t LDA & 56 \\\\\n",
       "\t QDA & 60 \\\\\n",
       "\t KNN & 54 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "name | acc | \n",
       "|---|---|---|---|\n",
       "| Logistic Regression | 56                  | \n",
       "| LDA | 56  | \n",
       "| QDA | 60  | \n",
       "| KNN | 54  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     name                acc\n",
       "[1,] Logistic Regression 56 \n",
       "[2,] LDA                 56 \n",
       "[3,] QDA                 60 \n",
       "[4,] KNN                 54 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy=cbind(name=\"Logistic Regression\",acc=round(mean(glmpred==Direction.2005)*100))\n",
    "accuracy=rbind(accuracy,list(name=\"LDA\",acc=round(mean(ldapred$class==Direction.2005)*100)))\n",
    "accuracy=rbind(accuracy,list(name=\"QDA\",acc=round(mean(qdaclass==Direction.2005)*100)))\n",
    "accuracy=rbind(accuracy,list(name=\"KNN\",acc=round(mean(knnpred==Direction.2005)*100)))\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN on Caravan Dataset\n",
    "\n",
    "Dataset has 85 predictors to measure demographics for 5822 people. Response variable is Purchase, indicates whether the person bought a Caravan insurance. In the dataset, only 6% people actually bought it.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(Caravan)\n",
    "attach(Caravan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>No</dt>\n",
       "\t\t<dd>5474</dd>\n",
       "\t<dt>Yes</dt>\n",
       "\t\t<dd>348</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[No] 5474\n",
       "\\item[Yes] 348\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "No\n",
       ":   5474Yes\n",
       ":   348\n",
       "\n"
      ],
      "text/plain": [
       "  No  Yes \n",
       "5474  348 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(Purchase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN predicts the class using the Euclidean distance. So scale matters. For example, if there are predictor variables like salary and age, then a difference of $1000 in salary far outweights a difference of 70y in age.\n",
    "\n",
    "So if you do a KNN considering salary and age, then salary will dominate the prediction results, age will be pretty much ignored in comparison.\n",
    "\n",
    "Yet in real life, a difference of 70y is a much bigger difference in relative terms than a $1000 diff in salary\n",
    "\n",
    "So we should standardize the variables. The `scale` function does that. It manipulates all variables so they have a Mean of 0 and an SD of 1. We'll do it for all columns except the 86th (which is the Purchase variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "165.037847395189"
      ],
      "text/latex": [
       "165.037847395189"
      ],
      "text/markdown": [
       "165.037847395189"
      ],
      "text/plain": [
       "[1] 165.0378"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.164707781931954"
      ],
      "text/latex": [
       "0.164707781931954"
      ],
      "text/markdown": [
       "0.164707781931954"
      ],
      "text/plain": [
       "[1] 0.1647078"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "standardized.X = scale(Caravan[,-86])\n",
    "var(Caravan[,1])\n",
    "var(Caravan[,2])\n",
    "var(standardized.X[,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=1:1000\n",
    "train.X=standardized.X[-test,]\n",
    "test.X=standardized.X[test,]\n",
    "train.Y=Purchase[-test]\n",
    "test.Y=Purchase[test]\n",
    "\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function that will help in running KNN for multiple K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doknn=function(k){\n",
    "    knnpred=knn(train.X,test.X,train.Y,k=k)\n",
    "    cat(\"K=\",k,\"\\n\")\n",
    "    cat(\"Error Rate of prediction:\\t\\t\",mean(test.Y!=knnpred),\"\\n\\n\")   # Error rate of prediction\n",
    "    print(table(test.Y,knnpred))\n",
    "    cat(\"\\nPPV (KNN,k=\",k,\"):\\t\\t\",sum(test.Y==\"Yes\" & knnpred ==\"Yes\")*100/sum(knnpred==\"Yes\"),\"%\\n\")\n",
    "    cat(\"PPV (always predict 'Yes'):\\t\",sum(test.Y==\"Yes\")*100/length(test.Y),\"%\")    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 1 \n",
      "Error Rate of prediction:\t\t 0.118 \n",
      "\n",
      "      knnpred\n",
      "test.Y  No Yes\n",
      "   No  873  68\n",
      "   Yes  50   9\n",
      "\n",
      "PPV (KNN,k= 1 ):\t\t 11.68831 %\n",
      "PPV (always predict 'Yes'):\t 5.9 %"
     ]
    }
   ],
   "source": [
    "doknn(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So blindly predicting \"No\" gives an error rate of 6%, but KNN gives an error of 12%!\n",
    "\n",
    "But let's say that there's a cost for pitching insurance to a customer. So, our objective is to control the cost - which means we must get it right when we say somebody will buy insurance. In other words, Positive Predictive Value (True Positive/Total Predicted Positive) should be maximized. Let's analyze that\n",
    "\n",
    "\n",
    "\n",
    "So 12% of the people predicted by KNN will actually end up buying insurance. As opposed to 6% if we blindly predicted \"Yes\". That's some improvement\n",
    "\n",
    "Now let's try with K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 3 \n",
      "Error Rate of prediction:\t\t 0.073 \n",
      "\n",
      "      knnpred\n",
      "test.Y  No Yes\n",
      "   No  922  19\n",
      "   Yes  54   5\n",
      "\n",
      "PPV (KNN,k= 3 ):\t\t 20.83333 %\n",
      "PPV (always predict 'Yes'):\t 5.9 %"
     ]
    }
   ],
   "source": [
    "doknn(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better! Let's try with K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 5 \n",
      "Error Rate of prediction:\t\t 0.066 \n",
      "\n",
      "      knnpred\n",
      "test.Y  No Yes\n",
      "   No  930  11\n",
      "   Yes  55   4\n",
      "\n",
      "PPV (KNN,k= 5 ):\t\t 26.66667 %\n",
      "PPV (always predict 'Yes'):\t 5.9 %"
     ]
    }
   ],
   "source": [
    "doknn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try Logistic Regression, with cutoffs at 0.5 and 0.25. TO begin with, let's create a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_glm = function(cutoff){\n",
    "    glm.fit=glm(Purchase~.,data=Caravan,family=binomial,subset=-test)\n",
    "    \n",
    "    glm.probs=predict(glm.fit,Caravan[test,],type=\"response\")\n",
    "    \n",
    "    glm.pred=rep(\"No\",1000)\n",
    "    \n",
    "    glm.pred[glm.probs>cutoff]=\"Yes\"\n",
    "    \n",
    "    cat(\"Cutoff=\",cutoff)\n",
    "    cat(\"\\n\")\n",
    "    print(table(glm.pred,test.Y))\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    cat(\"\\nPPV (Cutoff=\",cutoff,\"):\\t\\t\",\n",
    "        sum(test.Y==\"Yes\" & glm.pred ==\"Yes\")*100/sum(glm.pred==\"Yes\"),\"%\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff= 0.5\n",
      "        test.Y\n",
      "glm.pred  No Yes\n",
      "     No  934  59\n",
      "     Yes   7   0\n",
      "\n",
      "\n",
      "PPV (Cutoff= 0.5 ):\t\t 0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff= 0.25\n",
      "        test.Y\n",
      "glm.pred  No Yes\n",
      "     No  919  48\n",
      "     Yes  22  11\n",
      "\n",
      "\n",
      "PPV (Cutoff= 0.25 ):\t\t 33.33333 %\n"
     ]
    }
   ],
   "source": [
    "do_glm(0.5)\n",
    "do_glm(0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
