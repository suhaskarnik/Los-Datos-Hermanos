{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection\n",
    "\n",
    "Identify a subset of the *p* predictors that are believed to be related with the response. Then do an `lm` against those *p* predictors.\n",
    "\n",
    "But before that, let's define the term $R^2$\n",
    "\n",
    "Say that $\\bar{y}$ is the mean of the observed data. Then,\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "\n",
    "Then total Sum of squares is proportional to the variance of the data\n",
    "\n",
    "$$SS_{tot} = \\sum_{i=1}^n(y_i-\\bar{y})^2$$\n",
    "\n",
    "And the residual sum of squares (RSS) is sum of squares of all errors\n",
    "\n",
    "$$SS_{res} = RSS = \\sum_{i=1}^n(y_i - \\hat y_i)^2 = \\sum_{i=1}^n e_i^2$$\n",
    "\n",
    "Where $\\hat y_i$ is the predicted value for $y_i$ and therefore, $e_i$ is the $i^{th}$ error term.\n",
    "\n",
    "Given the above, $R^2$ is defined as \n",
    "\n",
    "$$R^2 \\equiv 1- \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "So, if the RSS is high, then $R^2$ goes down. For more info, check [wiki](https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Best Subset Selection\n",
    "\n",
    "Fit a separate Lin.Reg for each possible combination of the *p* predictors. Then look at all the models and identify the *best*\n",
    "\n",
    "That is, we fit all *p* models that contain one predictor, then fit all $( ^p_k ) = \\frac{p(p-1)}{2}$ models that contain all models with exactly two predictors etc.\n",
    "\n",
    "<u>**Algorithm**</u>\n",
    "1. Let $M_0$ denote the *null model*, which contains no predictors. This model simply predicts the sample mean for each observation\n",
    "<br><br>\n",
    "2. For $k$=1,2,3...*$p$*:\n",
    "    1. Fit all $\\binom{p}{k}$ models that contain exactly *k* predictors\n",
    "    2. Pick the best among these $\\binom{p}{k}$ models and call it $M_k$. Here, *best* is defined as the one that the least RSS, or the highest $R^2$\n",
    "<br><br>\n",
    "3. Select a single best model from $M_0...M_p$ using cross-validated prediction error, $C_p, AIC, BIC$ or adjusted $R^2$\n",
    "\n",
    "Step 2 identifies the best model for each subset size. In that step, $R^2$ increases monotonically as the number of features increases. So we will always end up with a model that includes all features. \n",
    "\n",
    "However, a high $R^2$ indicates a model with a low *training error*, but we want to minimize *test errors*. This is why Step 3, cross-validated prediction error is used.\n",
    "\n",
    "The same ideas apply to other models, like Log Reg. However, instead of using RSS there, we use *Deviance*, which plays the role of RSS for a broader class of models. The deviance is negative two times the maximized log-likelihood; the smaller the deviance, the better the fit.\n",
    "\n",
    "#### Computational Performance\n",
    "\n",
    "Obviously, this suffers from computational complexity. The number of possible models to evaluate grows rapidly with *p*. In general, there are $2^p$ models possible. Here's how that grows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIUEAIUEAIUEAIUEAIUEAIUGA/wAaXuucdLps1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(seq(1:25),2^seq(1:25),type = \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hit 1 Mil possibilities at 20 predictors. So this is extremely expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Selection\n",
    "\n",
    "When *p* is large, Best Subset Selection is too expensive. But it also suffers from statistical problems. The larger the search space (set of possibile combinations tested), the higher chance of finding models that work well against training data. We could land up overfitting the data and high variance\n",
    "\n",
    "#### Forward Stepwise Selection (FSS)\n",
    "\n",
    "Begins with a no-predictor model, then adds predictors one-by-one\n",
    "\n",
    "<u>**Algorithm**</u>\n",
    "\n",
    "1. Let $M_0$ denote the *null model*, which contains no predictors. This model simply predicts the sample mean for each observation\n",
    "<br><br>\n",
    "2. For $k$=0,1,2...,*$p-1$*:\n",
    "    1. Consider all *$p-k$* models to augment the predictors in $M_k$ with one additional predictor\n",
    "    2. Choose the best among these *$p-k$* models and call it $M_{k+1}$. Best is again defined as lowest RSS or highest $R^2$\n",
    "<br><br>\n",
    "3. Select a single best model from $M_0...M_p$ using cross-validated prediction error, $C_p, AIC, BIC$ or adjusted $R^2$\n",
    "\n",
    "\n",
    "Unlike BSS which needed to fit $2^p$ models, Forward Stepwise Selection requires fitting 1 NULL model, plus $p-k$ models per iteration. Totally, $1+\\sum_{k=0}^{p-1}(p-k)=1+ \\frac{p(p+1)}{2}$\n",
    "\n",
    "So for $p=20$, BSS fits $2^{20}$ (1 Mil+) models, FSS fits only 211 models\n",
    "\n",
    "However: The logic for FSS for essentially builds each level on the best predictor found on the previous level. So consider a dataset with 3 predictors, wherein *actually* the best *possible* one-variable model contains $X_1$ while the best *possible* two-variable model is $(X_2,X_3)$. \n",
    "\n",
    "Then FSS will not get the best possible two variable model. Because it will find $X_1$ to be the best 1-variable model, and therefore will search for the best model only among $(X_1,X_2)$ and $(X_1,X_3)$. The logic of FSS forces it to consider $X_1$ for each 2-variable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Selection (BaSS)\n",
    "\n",
    "Inverse of FSS. Starts with a model of *p* predictors. Then works *backwards*, by removing the least useful predictor.\n",
    "\n",
    "This only works if *$n \\geq p$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid\n",
    "\n",
    "Involves FSS and BaSS. As in FSS, adds new variables sequentially. But after adding each new variable, it may also remove variables that don't provide an improvement. This mimics BSS but without increasing computational overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Optimal Model\n",
    "\n",
    "So far we've talked about RSS and $R^2$. However, the model with all predictors will always have the highest $R^2$, because these quantities are related to the *training error*. But we want to minimize the *test error*. So these measures are not appropriate.\n",
    "\n",
    "To select the model with the minimum test error, we need to first estimate the test error. Two approaches:\n",
    "\n",
    "1. Indirectly estimate it by making an *adjustment* to the training error\n",
    "2. Directly estimate it using a validation set/cross-validation approach\n",
    "\n",
    "### $C_p$, AIC, BIC, Adjusted $R^2$\n",
    "\n",
    "The training set MSE is usually an under-estimate of the testing set MSE. That's because when we fit a model to the training data, it estimates the coefficients specifically to minimise the training MSE.\n",
    "\n",
    "But we can adjust the training error for a given model size\n",
    "\n",
    "##### $C_p$ Estimate\n",
    "For a fitted LM containing *d* predictors, the $C_p$ estimate of test MSE is:\n",
    "$$C_p = \\frac{1}{n}(RSS + 2d\\hat \\sigma^2)$$\n",
    "\n",
    "Where $\\hat \\sigma^2$ is an estimate of the variance error $\\epsilon$ for each response measurement. In other words, $C_p$ adds a penalty of $2d\\hat \\sigma^2$ to compensate for the fact that training error underestimates test error. As $C_p$ is proportional to the error, a low $C_p$ is preferred\n",
    "\n",
    "\n",
    "##### AIC Estimate\n",
    "Akaike information criterion (AIC) is given by \n",
    "\n",
    "$$AIC = \\frac{1}{n\\hat\\sigma^2}(RSS + 2d\\hat \\sigma^2)$$\n",
    "\n",
    "Rather similar to $C_p$, the only difference being that AIC is also inversely proportional to $\\hat\\sigma^2$. Here too, we seek to minimize AIC\n",
    "\n",
    "##### BIC Estimate\n",
    "Bayesian information criterion (BIC) is given by\n",
    "\n",
    "$$BIC = \\frac{1}{n}(RSS + log(n)d\\hat \\sigma^2)$$\n",
    "\n",
    "We seek to minimize BIC as well. Note: log(n)>2 for any n>7, so BIC places a heavier penalty on models with too many predictors.\n",
    "\n",
    "\n",
    "##### Adjusted $R^2$\n",
    "\n",
    "Recall: $R^2 = 1-\\frac{RSS}{TSS}$\n",
    "\n",
    "RSS always decreases as more variables are added, so $R^2$ always increases. For an LM with *d* variables, adjusted $R^2$ is calculated as:\n",
    "\n",
    "$$Adjusted R^2 = 1-\\frac{\\frac{RSS}{n-d-1}}{\\frac{TSS}{n-1}} = 1-\\frac{RSS\\times(n-1)}{TSS\\times(n-d-1)}$$\n",
    "\n",
    "Though RSS always increases with an addition of variables, $\\frac{RSS}{n-d-1}$ may increase or decrease depending on whether adding the *$d^{th}$* variable brings about a proportional decrease in RSS\n",
    "\n",
    "In this case a large $Adjusted R^2$, just like a large $R^2$, indicates a low test error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Shrinkage\n",
    "\n",
    "Fit a Lin.Reg for all *p* predictors. Then shrink the estimated coefficients towards 0 relative to the least square estimates. This is also called *regularization*. This will reduce variance. It may also reduce some coefficients to exactly 0, thereby performing variable selection\n",
    "\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Originally, Linear Reg works by reducing the RSS in the training data\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i - \\hat y_i)^2 = \\sum_{i=1}^n\\bigg(y_i - \\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\bigg)^2$$\n",
    "\n",
    "So Linear Reg works by choosing $\\beta_0, \\beta_1,...,\\beta_p$ to minimize the RSS\n",
    "\n",
    "Ridge Regression is similar, it chooses coefficients to minimize:\n",
    "\n",
    "$$RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "Where $\\lambda \\geq 0$ is a *tuning parameter*. The combined term $\\lambda\\sum_{j=1}^p\\beta_j^2$ is called a *shrinkage penalty*, and it is small when $\\beta_1,...,\\beta_p$ are close to zero, so it effectively shrinks the estimates of $\\beta_j$ towards zero. \n",
    "\n",
    "When $\\lambda=0$, then there is no shrinkage penalty. But as $\\lambda \\rightarrow \\infty$, impact of shrinkage grows, and coefficient estimates approach zero. So of course, selecting a good $\\lambda$ is important.\n",
    "\n",
    "This shrinkage penalty is not applied to $\\beta_0$, which is also why \"0\" doesnt appear in the sum term there. This is because we only want to shrink the est. association of each variable with the response.\n",
    "\n",
    "It is important to apply Ridge Reg only after standardizing the predictors.\n",
    "\n",
    "#### Why does it work?\n",
    "\n",
    "Ridge Reg uses the Bias-Variance trade-off. As $\\lambda$ increases, the flexibility of the regression fit decreases, so variance decreases while bias increases.\n",
    "\n",
    "Hence, it works best when the LM estimates have high variance.\n",
    "\n",
    "\n",
    "### Lasso\n",
    "\n",
    "However, Ridge Reg has a disadvantage: it will contain all *p* predictors. It may shrink some of them to almost (but not exactly) zero. This doesn't affect accuracy, but reduces interpretability and makes the model harder to explain.\n",
    "\n",
    "*Lasso coefficients* try to minimize the quantity:\n",
    "\n",
    "$$ \\begin{equation} \\sum_{i=1}^n\\bigg( y_i - \\beta_0-\\sum_{j=1}^p\\beta_jx_{ij} \\bigg)^2 + \\lambda\\sum_{j=1}^p |\\beta_j| = RSS + \\lambda\\sum_{j=1}^p |\\beta_j| \\end{equation}$$\n",
    "\n",
    "This also shrinks the coefficient estimates towards zero. But it has the effect of making some coefficients *exactly* zero. This is also known as a *sparse* model - one that contains a subset of predictors.\n",
    "\n",
    "\n",
    "### Comparison\n",
    "\n",
    "Lasso works better when the number of true predictors are relatively smaller. In practice, it is better to use cross-validation to identify whether Lasso or Ridge Reg is better for a specific model.\n",
    "\n",
    "\n",
    "### Selecting the tuning parameter\n",
    "\n",
    "Create a grid of $\\lambda$ values, compute the cross-validation error for them. Then select the one which has the lowest X-validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Dimension Reduction\n",
    "\n",
    "*Project* the *p* predictors to an *M*-dimensional subspace, where *$M<p$*\n",
    "\n",
    "Compute *M* different linear combinations, or *projections* on the variables. Then these *M* are used as predictors to fit the linear model. \n",
    "\n",
    "The predictors are transformed and then an LM is done using transformed variables.\n",
    "\n",
    "Let $Z_1,Z_2,...,Z_M$ be $M<p$ *linear combinations* of the original *p* predictors. So:\n",
    "\n",
    "$$Z_m = \\sum_{j=1}^p\\phi_{jm} X_j$$\n",
    "\n",
    "Or, $$Z =\n",
    "\\begin{bmatrix}\n",
    "\\phi_{11},\\phi_{21},...,\\phi_{p1} \\\\\n",
    "\\phi_{12},\\phi_{22},...,\\phi_{p2} \\\\\n",
    "...,...,...,... \\\\\n",
    "\\phi_{1M},\\phi_{2M},...,\\phi_{pM}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_p\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "As $p<m$, the first matrix above is a rectangular one. And a rectangular matrix with fewer rows than columns squishes a higher-dimensional space into a lower-dimensional space. Which is the reduction of dimensionality.\n",
    "\n",
    "For some constants $\\phi_{1m},\\phi_{2m},...,\\phi_{pm}, m=1,...,M$\n",
    "\n",
    "Then we fit the linear model: \n",
    "\n",
    "$$y_i=\\theta_0 + \\sum_{m=1}^M \\theta_mz_{im} + \\epsilon_i \\\\\n",
    "i=1,...,n$$\n",
    "\n",
    "If the constants are chosen correctly, fitting the above using least squares can outperform fitting the original model using least squares.\n",
    "\n",
    "Dimension Reduction constrains the estimated $\\beta_j$ coefficients. This constraint can increase the bias of the model. But when *p* is relatively large compared to *n*, selecting a value of $M \\ll p$ will greatly reduce the variance.\n",
    "\n",
    "\n",
    "### Principal Components Regression or Principal Components Analysis (PCA)\n",
    "\n",
    "PCA is also used for unsupervised learning, but here let's focus on Dimension reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
