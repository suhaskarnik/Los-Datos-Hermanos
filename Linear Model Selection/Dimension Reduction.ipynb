{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "*Project* the *p* predictors to an *M*-dimensional subspace, where *$M<p$*\n",
    "\n",
    "Compute *M* different linear combinations, or *projections* on the variables. Then these *M* are used as predictors to fit the linear model. \n",
    "\n",
    "The predictors are transformed and then an LM is done using transformed variables.\n",
    "\n",
    "Let $Z_1,Z_2,...,Z_M$ be $M<p$ *linear combinations* of the original *p* predictors. So:\n",
    "\n",
    "$$\\begin{equation} Z_m = \\sum_{j=1}^p\\phi_{jm} X_j\\end{equation}$$\n",
    "\n",
    "Or, $$Z =\n",
    "\\begin{bmatrix}\n",
    "\\phi_{11},\\phi_{21},...,\\phi_{p1} \\\\\n",
    "\\phi_{12},\\phi_{22},...,\\phi_{p2} \\\\\n",
    "...,...,...,... \\\\\n",
    "\\phi_{1M},\\phi_{2M},...,\\phi_{pM}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "...\\\\\n",
    "x_p\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "As $p<m$, the first matrix above is a rectangular one. And a rectangular matrix with fewer rows than columns squishes a higher-dimensional space into a lower-dimensional space. Which is the reduction of dimensionality.\n",
    "\n",
    "For some constants $\\phi_{1m},\\phi_{2m},...,\\phi_{pm}, m=1,...,M$\n",
    "\n",
    "Then we fit the linear model: \n",
    "\n",
    "$$y_i=\\theta_0 + \\sum_{m=1}^M \\theta_mz_{im} + \\epsilon_i \\\\\n",
    "i=1,...,n$$\n",
    "\n",
    "If the constants are chosen correctly, fitting the above using least squares can outperform fitting the original model using least squares.\n",
    "\n",
    "Dimension Reduction constrains the estimated $\\beta_j$ coefficients. This constraint can increase the bias of the model. But when *p* is relatively large compared to *n*, selecting a value of $M \\ll p$ will greatly reduce the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Regression (PCR) or Principal Components Analysis (PCA)\n",
    "\n",
    "PCA is mainly used for *unsupervised* learning, but here let's focus on Dimension reduction.\n",
    "\n",
    "The First Principal Component direction ($Z_1$) of the data is the one along which the observations *vary* the most.\n",
    "\n",
    "The Second Principal Component direction ($Z_2$) is a linear combination of variables uncorrelated with with $Z_1$, and has the highest variance subject to that constraint\n",
    "\n",
    "### Principal Components Regression Approach\n",
    "\n",
    "Construct the first $M$ principal components $Z_1,Z_2,...,Z_M$.\n",
    "\n",
    "The assumption (possibly a source of bias) is that a small number of Principal Components explain most of the variability of the data AND the relationship with the response.\n",
    "\n",
    "If that assumption holds, then fitting a LM to $Z_1, Z_2,...,Z_M$ gives better results than fitting to $X_1,X_2,...,X_p$, as most of the information related to response is contained in Z. By fitting fewer variables, we also mitigate overfitting.\n",
    "\n",
    "However, it is **not** a feature selection method. This is because each principal component is a *linear combination* of the variables.\n",
    "\n",
    "The number of Principal Components($M$) is chosen by cross-validation. It is good to standardize the variables to ensure that they are all on the same scale before going for PCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares (PLS)\n",
    "\n",
    "PCR is an unsupervised approach. That is, the response doesn't supervise the identification of princpal components. Therefore in PCR, there is no guarantee that the directions that best explain the variance are also the ones that predict the response. \n",
    "\n",
    "Identifies a set of features $Z_1,Z_2,...,Z_M$ that are linear combinations of the original features. Then fits a LM using M features. But the new features are obtained in a *supervised* way.\n",
    "\n",
    "After standardizing the $p$ predictors, PLS calculates the first direction $Z_1$ by setting each $\\phi_{j1}$ equal to the coefficient from the simple linear regression of $Y$ onto $X_j$. This coefficient is proportional to correlation between $Y$ and $X_j$. So in computing $Z_1 = \\sum_{j=1}^p\\phi_{j1}X_j$, it places the highest weight on variables that are most strongly related to the response.\n",
    "\n",
    "So first, PLS picks up the direction $Z_1$ with the highest correlation to the response. Then, it identifies the second direction by first adjusting each variable for $Z_1$ by regressing each variable for $Z_1$ and taking residuals. Of course, residuals are the remaining info that has not been explained by the first PLS direction. Then $Z_2$ is computed on this $orthogonalized$ dataset just like $Z_1$. And so on it goes $M$ times.\n",
    "\n",
    "This $M$ is chosen by cross-validation. In practice this is no better than PCR or Ridge Regression. The fact that this is supervised reduces bias from PCR, but obviously in exchange for an increase in variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerations in High Dimensions\n",
    "\n",
    "Classical least-squares approaches are appropriate only when $n\\gg p$. When $n < p$ or even when $n > p$, the issues o bias-variance trade-off and overfitting become more pronounced.\n",
    "\n",
    "When $p$ is as large as or larger than $n$, then least-squares should not be done. Because:\n",
    "\n",
    "1. Even if there's no relationship between the features and the response, least-squares will give estimates that are a perfect fit to the data, such that residuals are zero\n",
    "2. An extreme case: consider two variables and two observations. Least squares will give a \"perfect fit\" because it can easily draw a line between the observations. \n",
    "3. The $R^2$ rises when the variables increase, so naively looking at $R^2$ would give misleading results.\n",
    "\n",
    "The lasso, least squares, forward stepwise selection etc reduce the variance (and therefore overfitting) by using a less flexible approach. Yes, that increases bias, but variance is the key problem here. \n",
    "\n",
    "1. Regularization/shrinkage can play a key role in high-dimensional cases\n",
    "2. A good tuning parameter ($\\lambda$) should be chosen\n",
    "3. Test error increases with number of predictors\n",
    "\n",
    "The third point is because adding relevant signal features improves predictive power. Adding random noise features decreases predictive power. Even when new features are relevant, the rise in variance due to including them can outweigh the improvement in bias.\n",
    "\n",
    "Also consider multicollinearity wherein some predictors are positively correlated. In high-dimensions, any variable can be written as a linear combination of the others. So which variable is truly connected to the response is hard to ascertain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
