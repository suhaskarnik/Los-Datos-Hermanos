{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage\n",
    "\n",
    "Fit a Lin.Reg for all *p* predictors. Then shrink the estimated coefficients towards 0 relative to the least square estimates. This is also called *regularization*. This will reduce variance. It may also reduce some coefficients to exactly 0, thereby performing variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use `glmnet` package. The main function is also called `glmnet()`\n",
    "\n",
    "As an input, that function expects:\n",
    "* `x`: a matrix, where each row is an obs vector\n",
    "* `y`: a response variable, quantitative for `family=gaussian|poisson`. For `family=binomial`, it should be either factor with 2 levels or a 2-col matrix of counts (logistical regression)\n",
    "* `family`: as above\n",
    "\n",
    "First clean out the model data of NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'glmnet' was built under R version 3.3.3\"Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Warning message:\n",
      "\"package 'foreach' was built under R version 3.3.3\"Loaded glmnet 2.0-5\n",
      "\n",
      "Warning message:\n",
      "\"package 'ISLR' was built under R version 3.3.2\""
     ]
    },
    {
     "data": {
      "text/html": [
       "59"
      ],
      "text/latex": [
       "59"
      ],
      "text/markdown": [
       "59"
      ],
      "text/plain": [
       "[1] 59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(glmnet)\n",
    "library(ISLR)\n",
    "fix(Hitters)\n",
    "\n",
    "sum(is.na(Hitters$Salary))  # No. of null Salaries\n",
    "\n",
    "Hitters=na.omit(Hitters)\n",
    "sum(is.na(Hitters$Salary))  # No. of null Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a matrix, remove the first column, which contains the Intercept\n",
    "# We are removing this \n",
    "\n",
    "# model.matrix creates a matrix for the 19 predictors, but it also converts qualitative vars into dummy vars. \n",
    "#   this is important because glmnet can only take quant inputs\n",
    "\n",
    "x=model.matrix(Salary~.,Hitters)[,-1]\n",
    "y=Hitters$Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Originally, Linear Reg works by reducing the RSS in the training data\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i - \\hat y_i)^2 = \\sum_{i=1}^n\\bigg(y_i - \\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\bigg)^2$$\n",
    "\n",
    "So Linear Reg works by choosing $\\beta_0, \\beta_1,...,\\beta_p$ to minimize the RSS\n",
    "\n",
    "Ridge Regression is similar, it chooses coefficients to minimize:\n",
    "\n",
    "$$RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
    "\n",
    "Where $\\lambda \\geq 0$ is a *tuning parameter*. The combined term $\\lambda\\sum_{j=1}^p\\beta_j^2$ is called a *shrinkage penalty*, and it is small when $\\beta_1,...,\\beta_p$ are close to zero, so it effectively shrinks the estimates of $\\beta_j$ towards zero. \n",
    "\n",
    "When $\\lambda=0$, then there is no shrinkage penalty. But as $\\lambda \\rightarrow \\infty$, impact of shrinkage grows, and coefficient estimates approach zero. So of course, selecting a good $\\lambda$ is important.\n",
    "\n",
    "This shrinkage penalty is not applied to $\\beta_0$, which is also why \"0\" doesnt appear in the sum term there. This is because we only want to shrink the est. association of each variable with the response.\n",
    "\n",
    "It is important to apply Ridge Reg only after standardizing the predictors.\n",
    "\n",
    "## $\\ell_2$ Norm\n",
    "\n",
    "This is a related concept, and is denoted as $||\\beta||_2$, and denoted as \n",
    "$$\\ell_2 = ||\\beta||_2 = \\sqrt{ \\sum_{j=1}^p \\beta_j^2 }$$\n",
    "\n",
    "IOW, $\\ell_2$ is the Mean Square of the coefficient estimates for all coefs except the intercept.\n",
    "\n",
    "As $\\lambda$ increases, the $\\ell_2$ norm will always decrease, and so will $$\\frac{||\\hat{\\beta}_\\lambda^R||_2}{||\\hat{\\beta}||_2}$$\n",
    "\n",
    "Which is a fancy way of saying that as $\\lambda$ increases, the Mean Square of ridge reg coefficients decreases, which is why the ratio of that Mean Square to the Mean Square of the original coefficients.\n",
    "\n",
    "**Note**: In linear algebra, and related areas of mathematics, a norm is a function that assigns a strictly positive length or size to each vector in a vector space\n",
    "\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "Ridge Reg uses the Bias-Variance trade-off. As $\\lambda$ increases, the flexibility of the regression fit decreases, so variance decreases while bias increases.\n",
    "\n",
    "Hence, it works best when the LM estimates have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glmnet()` has an `alpha` arg which determines what type of model is fit. When `alpha=0`, then Ridge Reg is fit, and when `alpha=1` then Lasso is fit. By default, it also standardizes the variables so they are all on the same scale. This can be disabled by setting `standardize=FALSE`.\n",
    "\n",
    "Line 1 below generates a vector called `grid` for $\\lambda$ values. This `grid` contains `100` values ranging from $10^{10}$ to $10^{-2}$. Note: all those values need not be powers of 10.\n",
    "\n",
    "On running `glmnet()`, we get a vector of ridge reg coefficients. Here its a $20\\times100$ matrix, having 20 rows (1 per predictor + intercept) and 100 columns (one for each $\\lambda$ in our `grid`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>20</li>\n",
       "\t<li>100</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 20\n",
       "\\item 100\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 20\n",
       "2. 100\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  20 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid=10^seq(10,-2,length=100)\n",
    "\n",
    "ridge.mod=glmnet(x,y,alpha=0,lambda = grid)\n",
    "\n",
    "dim(coef(ridge.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\lambda$ is high, the coefficients will be much smaller in terms of the $\\ell_2$ norm.\n",
    "\n",
    "Here are the coefs with their $\\ell_2$ norms. Recall that $\\lambda$ was decreasing in `grid`, so the earlier indices have the higher $\\lambda$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "11497.5699539774"
      ],
      "text/latex": [
       "11497.5699539774"
      ],
      "text/markdown": [
       "11497.5699539774"
      ],
      "text/plain": [
       "[1] 11497.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6.36061242142791"
      ],
      "text/latex": [
       "6.36061242142791"
      ],
      "text/markdown": [
       "6.36061242142791"
      ],
      "text/plain": [
       "[1] 6.360612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'=============================='"
      ],
      "text/latex": [
       "'=============================='"
      ],
      "text/markdown": [
       "'=============================='"
      ],
      "text/plain": [
       "[1] \"==============================\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "705.480231071865"
      ],
      "text/latex": [
       "705.480231071865"
      ],
      "text/markdown": [
       "705.480231071865"
      ],
      "text/plain": [
       "[1] 705.4802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "57.110014262533"
      ],
      "text/latex": [
       "57.110014262533"
      ],
      "text/markdown": [
       "57.110014262533"
      ],
      "text/plain": [
       "[1] 57.11001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19 x 2 sparse Matrix of class \"dgCMatrix\"\n",
       "                    s49          s59\n",
       "AtBat       0.036957182   0.11211115\n",
       "Hits        0.138180344   0.65622409\n",
       "HmRun       0.524629976   1.17980910\n",
       "Runs        0.230701523   0.93769713\n",
       "RBI         0.239841459   0.84718546\n",
       "Walks       0.289618741   1.31987948\n",
       "Years       1.107702929   2.59640425\n",
       "CAtBat      0.003131815   0.01083413\n",
       "CHits       0.011653637   0.04674557\n",
       "CHmRun      0.087545670   0.33777318\n",
       "CRuns       0.023379882   0.09355528\n",
       "CRBI        0.024138320   0.09780402\n",
       "CWalks      0.025015421   0.07189612\n",
       "LeagueN     0.085028114  13.68370191\n",
       "DivisionW  -6.215440973 -54.65877750\n",
       "PutOuts     0.016482577   0.11852289\n",
       "Assists     0.002612988   0.01606037\n",
       "Errors     -0.020502690  -0.70358655\n",
       "NewLeagueN  0.301433531   8.61181213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge.mod$lambda[50]\n",
    "sqrt(sum(coef(ridge.mod)[-1,50]^2))   # ell_2 \n",
    "#coef(ridge.mod)[,50]\n",
    "\n",
    "paste(rep(\"=\",30),collapse = \"\")\n",
    "\n",
    "ridge.mod$lambda[60]\n",
    "sqrt(sum(coef(ridge.mod)[-1,60]^2))   # ell_2\n",
    "\n",
    "coef(ridge.mod)[-1,c(50,60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict()` function can be used to obtain the ridge reg for a new $\\lambda$ value, say `50`. This is done by the `s=50` arg. Not surprisingly the coefs for this low $\\lambda$ value are much higher (in abs terms) than the coefs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>AtBat</dt>\n",
       "\t\t<dd>-0.358099859376738</dd>\n",
       "\t<dt>Hits</dt>\n",
       "\t\t<dd>1.96935928646357</dd>\n",
       "\t<dt>HmRun</dt>\n",
       "\t\t<dd>-1.27824798145678</dd>\n",
       "\t<dt>Runs</dt>\n",
       "\t\t<dd>1.14589163211962</dd>\n",
       "\t<dt>RBI</dt>\n",
       "\t\t<dd>0.803829228437672</dd>\n",
       "\t<dt>Walks</dt>\n",
       "\t\t<dd>2.71618579623371</dd>\n",
       "\t<dt>Years</dt>\n",
       "\t\t<dd>-6.21831921727865</dd>\n",
       "\t<dt>CAtBat</dt>\n",
       "\t\t<dd>0.00544783719814918</dd>\n",
       "\t<dt>CHits</dt>\n",
       "\t\t<dd>0.10648951402342</dd>\n",
       "\t<dt>CHmRun</dt>\n",
       "\t\t<dd>0.624485956082661</dd>\n",
       "\t<dt>CRuns</dt>\n",
       "\t\t<dd>0.221498463760022</dd>\n",
       "\t<dt>CRBI</dt>\n",
       "\t\t<dd>0.218691380321248</dd>\n",
       "\t<dt>CWalks</dt>\n",
       "\t\t<dd>-0.150024548516927</dd>\n",
       "\t<dt>LeagueN</dt>\n",
       "\t\t<dd>45.9258855144158</dd>\n",
       "\t<dt>DivisionW</dt>\n",
       "\t\t<dd>-118.201136816368</dd>\n",
       "\t<dt>PutOuts</dt>\n",
       "\t\t<dd>0.250232154092559</dd>\n",
       "\t<dt>Assists</dt>\n",
       "\t\t<dd>0.121566461346767</dd>\n",
       "\t<dt>Errors</dt>\n",
       "\t\t<dd>-3.27859954463555</dd>\n",
       "\t<dt>NewLeagueN</dt>\n",
       "\t\t<dd>-9.4966803100264</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[AtBat] -0.358099859376738\n",
       "\\item[Hits] 1.96935928646357\n",
       "\\item[HmRun] -1.27824798145678\n",
       "\\item[Runs] 1.14589163211962\n",
       "\\item[RBI] 0.803829228437672\n",
       "\\item[Walks] 2.71618579623371\n",
       "\\item[Years] -6.21831921727865\n",
       "\\item[CAtBat] 0.00544783719814918\n",
       "\\item[CHits] 0.10648951402342\n",
       "\\item[CHmRun] 0.624485956082661\n",
       "\\item[CRuns] 0.221498463760022\n",
       "\\item[CRBI] 0.218691380321248\n",
       "\\item[CWalks] -0.150024548516927\n",
       "\\item[LeagueN] 45.9258855144158\n",
       "\\item[DivisionW] -118.201136816368\n",
       "\\item[PutOuts] 0.250232154092559\n",
       "\\item[Assists] 0.121566461346767\n",
       "\\item[Errors] -3.27859954463555\n",
       "\\item[NewLeagueN] -9.4966803100264\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "AtBat\n",
       ":   -0.358099859376738Hits\n",
       ":   1.96935928646357HmRun\n",
       ":   -1.27824798145678Runs\n",
       ":   1.14589163211962RBI\n",
       ":   0.803829228437672Walks\n",
       ":   2.71618579623371Years\n",
       ":   -6.21831921727865CAtBat\n",
       ":   0.00544783719814918CHits\n",
       ":   0.10648951402342CHmRun\n",
       ":   0.624485956082661CRuns\n",
       ":   0.221498463760022CRBI\n",
       ":   0.218691380321248CWalks\n",
       ":   -0.150024548516927LeagueN\n",
       ":   45.9258855144158DivisionW\n",
       ":   -118.201136816368PutOuts\n",
       ":   0.250232154092559Assists\n",
       ":   0.121566461346767Errors\n",
       ":   -3.27859954463555NewLeagueN\n",
       ":   -9.4966803100264\n",
       "\n"
      ],
      "text/plain": [
       "        AtBat          Hits         HmRun          Runs           RBI \n",
       "-3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00  8.038292e-01 \n",
       "        Walks         Years        CAtBat         CHits        CHmRun \n",
       " 2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01  6.244860e-01 \n",
       "        CRuns          CRBI        CWalks       LeagueN     DivisionW \n",
       " 2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 -1.182011e+02 \n",
       "      PutOuts       Assists        Errors    NewLeagueN \n",
       " 2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(ridge.mod ,s=50, type =\"coefficients\")[2:20 ,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "However, Ridge Reg has a disadvantage: it will contain all *p* predictors. It may shrink some of them to almost (but not exactly) zero. This doesn't affect accuracy, but reduces interpretability and makes the model harder to explain.\n",
    "\n",
    "*Lasso coefficients* try to minimize the quantity:\n",
    "\n",
    "$$ \\begin{equation} \\sum_{i=1}^n\\bigg( y_i - \\beta_0-\\sum_{j=1}^p\\beta_jx_{ij} \\bigg)^2 + \\lambda\\sum_{j=1}^p |\\beta_j| = RSS + \\lambda\\sum_{j=1}^p |\\beta_j| \\end{equation}$$\n",
    "\n",
    "This also shrinks the coefficient estimates towards zero. But it has the effect of making some coefficients *exactly* zero. This is also known as a *sparse* model - one that contains a subset of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the samples into a test and train set, and estimate the test error for Ridge Reg and Lasso.\n",
    "\n",
    "Let's try an alternative approach to do the splitting though, just for a change: Randomly choose a subset of numbers between 1 and n. Then this becomes the test set and the rest are training sets. This will split the data 50-50 into test and train - that's what the `nrow(x)/2` is for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "131"
      ],
      "text/latex": [
       "131"
      ],
      "text/markdown": [
       "131"
      ],
      "text/plain": [
       "[1] 131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "263"
      ],
      "text/latex": [
       "263"
      ],
      "text/markdown": [
       "263"
      ],
      "text/plain": [
       "[1] 263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "\n",
    "train=sample(1:nrow(x),nrow(x)/2)\n",
    "test=-train\n",
    "y.test=y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a ridge reg on the training set, evaluate its MSE on the test set using $\\lambda=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "101036.832669597"
      ],
      "text/latex": [
       "101036.832669597"
      ],
      "text/markdown": [
       "101036.832669597"
      ],
      "text/plain": [
       "[1] 101036.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)\n",
    "ridge.pred=predict(ridge.mod,s=4,newx=x[test,])\n",
    "mean((ridge.pred-y[test])^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So test MSE is 101K. Instead, if we had simply fit a model with just an intercept, we would have predicted each test obs using the mean of the training obs. So, the test MSE in that case would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "193253.113067991"
      ],
      "text/latex": [
       "193253.113067991"
      ],
      "text/markdown": [
       "193253.113067991"
      ],
      "text/plain": [
       "[1] 193253.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean((mean(y[train])-y.test)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result is also possible if we choose a *very high* value of $\\lambda$ (such as $10^{10}$), as that will drive most coefs to zero or close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "193253.05679545"
      ],
      "text/latex": [
       "193253.05679545"
      ],
      "text/markdown": [
       "193253.05679545"
      ],
      "text/plain": [
       "[1] 193253.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)\n",
    "ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])\n",
    "mean((ridge.pred-y[test])^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\lambda=4$ is definitely better (lower test MSE) than $\\lambda \\rightarrow \\infty$.\n",
    "\n",
    "On the other extreme, $\\lambda=0$ would give a model with the same coefs as vanilla least squares (i.e `lm`). Lets see the test MSE of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "114723.615178307"
      ],
      "text/latex": [
       "114723.615178307"
      ],
      "text/markdown": [
       "114723.615178307"
      ],
      "text/plain": [
       "[1] 114723.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>299.42848754</td><td>299.42883596</td></tr>\n",
       "\t<tr><th scope=row>xAtBat</th><td> -2.54027494</td><td> -2.54014665</td></tr>\n",
       "\t<tr><th scope=row>xHits</th><td>  8.36682108</td><td>  8.36611719</td></tr>\n",
       "\t<tr><th scope=row>xHmRun</th><td> 11.64512445</td><td> 11.64400720</td></tr>\n",
       "\t<tr><th scope=row>xRuns</th><td> -9.09922578</td><td> -9.09877719</td></tr>\n",
       "\t<tr><th scope=row>xRBI</th><td>  2.44104711</td><td>  2.44152119</td></tr>\n",
       "\t<tr><th scope=row>xWalks</th><td>  9.23440293</td><td>  9.23403909</td></tr>\n",
       "\t<tr><th scope=row>xYears</th><td>-22.93673265</td><td>-22.93584442</td></tr>\n",
       "\t<tr><th scope=row>xCAtBat</th><td> -0.18153945</td><td> -0.18160843</td></tr>\n",
       "\t<tr><th scope=row>xCHits</th><td> -0.11598196</td><td> -0.11561496</td></tr>\n",
       "\t<tr><th scope=row>xCHmRun</th><td> -1.33887794</td><td> -1.33836534</td></tr>\n",
       "\t<tr><th scope=row>xCRuns</th><td>  3.32838295</td><td>  3.32817777</td></tr>\n",
       "\t<tr><th scope=row>xCRBI</th><td>  0.07536488</td><td>  0.07511771</td></tr>\n",
       "\t<tr><th scope=row>xCWalks</th><td> -1.07841010</td><td> -1.07828647</td></tr>\n",
       "\t<tr><th scope=row>xLeagueN</th><td> 59.76065162</td><td> 59.76529059</td></tr>\n",
       "\t<tr><th scope=row>xDivisionW</th><td>-98.86233333</td><td>-98.85996590</td></tr>\n",
       "\t<tr><th scope=row>xPutOuts</th><td>  0.34086849</td><td>  0.34086400</td></tr>\n",
       "\t<tr><th scope=row>xAssists</th><td>  0.34164753</td><td>  0.34165605</td></tr>\n",
       "\t<tr><th scope=row>xErrors</th><td> -0.64207160</td><td> -0.64205839</td></tr>\n",
       "\t<tr><th scope=row>xNewLeagueN</th><td> -0.67441589</td><td> -0.67606314</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "\t(Intercept) & 299.42848754 & 299.42883596\\\\\n",
       "\txAtBat &  -2.54027494 &  -2.54014665\\\\\n",
       "\txHits &   8.36682108 &   8.36611719\\\\\n",
       "\txHmRun &  11.64512445 &  11.64400720\\\\\n",
       "\txRuns &  -9.09922578 &  -9.09877719\\\\\n",
       "\txRBI &   2.44104711 &   2.44152119\\\\\n",
       "\txWalks &   9.23440293 &   9.23403909\\\\\n",
       "\txYears & -22.93673265 & -22.93584442\\\\\n",
       "\txCAtBat &  -0.18153945 &  -0.18160843\\\\\n",
       "\txCHits &  -0.11598196 &  -0.11561496\\\\\n",
       "\txCHmRun &  -1.33887794 &  -1.33836534\\\\\n",
       "\txCRuns &   3.32838295 &   3.32817777\\\\\n",
       "\txCRBI &   0.07536488 &   0.07511771\\\\\n",
       "\txCWalks &  -1.07841010 &  -1.07828647\\\\\n",
       "\txLeagueN &  59.76065162 &  59.76529059\\\\\n",
       "\txDivisionW & -98.86233333 & -98.85996590\\\\\n",
       "\txPutOuts &   0.34086849 &   0.34086400\\\\\n",
       "\txAssists &   0.34164753 &   0.34165605\\\\\n",
       "\txErrors &  -0.64207160 &  -0.64205839\\\\\n",
       "\txNewLeagueN &  -0.67441589 &  -0.67606314\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| (Intercept) | 299.42848754 | 299.42883596 | \n",
       "| xAtBat |  -2.54027494 |  -2.54014665 | \n",
       "| xHits |   8.36682108 |   8.36611719 | \n",
       "| xHmRun |  11.64512445 |  11.64400720 | \n",
       "| xRuns |  -9.09922578 |  -9.09877719 | \n",
       "| xRBI |   2.44104711 |   2.44152119 | \n",
       "| xWalks |   9.23440293 |   9.23403909 | \n",
       "| xYears | -22.93673265 | -22.93584442 | \n",
       "| xCAtBat |  -0.18153945 |  -0.18160843 | \n",
       "| xCHits |  -0.11598196 |  -0.11561496 | \n",
       "| xCHmRun |  -1.33887794 |  -1.33836534 | \n",
       "| xCRuns |   3.32838295 |   3.32817777 | \n",
       "| xCRBI |   0.07536488 |   0.07511771 | \n",
       "| xCWalks |  -1.07841010 |  -1.07828647 | \n",
       "| xLeagueN |  59.76065162 |  59.76529059 | \n",
       "| xDivisionW | -98.86233333 | -98.85996590 | \n",
       "| xPutOuts |   0.34086849 |   0.34086400 | \n",
       "| xAssists |   0.34164753 |   0.34165605 | \n",
       "| xErrors |  -0.64207160 |  -0.64205839 | \n",
       "| xNewLeagueN |  -0.67441589 |  -0.67606314 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            [,1]         [,2]        \n",
       "(Intercept) 299.42848754 299.42883596\n",
       "xAtBat       -2.54027494  -2.54014665\n",
       "xHits         8.36682108   8.36611719\n",
       "xHmRun       11.64512445  11.64400720\n",
       "xRuns        -9.09922578  -9.09877719\n",
       "xRBI          2.44104711   2.44152119\n",
       "xWalks        9.23440293   9.23403909\n",
       "xYears      -22.93673265 -22.93584442\n",
       "xCAtBat      -0.18153945  -0.18160843\n",
       "xCHits       -0.11598196  -0.11561496\n",
       "xCHmRun      -1.33887794  -1.33836534\n",
       "xCRuns        3.32838295   3.32817777\n",
       "xCRBI         0.07536488   0.07511771\n",
       "xCWalks      -1.07841010  -1.07828647\n",
       "xLeagueN     59.76065162  59.76529059\n",
       "xDivisionW  -98.86233333 -98.85996590\n",
       "xPutOuts      0.34086849   0.34086400\n",
       "xAssists      0.34164753   0.34165605\n",
       "xErrors      -0.64207160  -0.64205839\n",
       "xNewLeagueN  -0.67441589  -0.67606314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)\n",
    "ridge.pred=predict(ridge.mod,s=0,newx=x[test,])\n",
    "mean((ridge.pred-y[test])^2)\n",
    "\n",
    "cbind(\n",
    "lm(y~x,subset=train)$coefficients,\n",
    "predict(ridge.mod,s=0,exact=T,type=\"coefficients\")[1:20,]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But choosing $\\lambda$ arbitrarily is not good practice. Let's use X-validation for this instead. There's a `cv.glmnet()` function that can do this for us. By default it does 10-fold X-validation, but that can be tuned by using the `nfolds` arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "#Pg 268 ISLR\n",
    "#cv.out=cv.glmnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Lasso works better when the number of true predictors are relatively smaller. In practice, it is better to use cross-validation to identify whether Lasso or Ridge Reg is better for a specific model.\n",
    "\n",
    "\n",
    "### Selecting the tuning parameter\n",
    "\n",
    "Create a grid of $\\lambda$ values, compute the cross-validation error for them. Then select the one which has the lowest X-validation error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
